<p align="center">
  <img src="SwarmTorch.png" alt="SwarmTorch logo" width="160" />
</p>

# SwarmTorch

[![Crates.io](https://img.shields.io/crates/v/swarm-torch.svg)](https://crates.io/crates/swarm-torch)
[![Documentation](https://docs.rs/swarm-torch/badge.svg)](https://docs.rs/swarm-torch)
[![License: MPL 2.0](https://img.shields.io/badge/License-MPL_2.0-brightgreen.svg)](LICENSE)
[![CI](https://github.com/swarmic/SwarmTorch/actions/workflows/rust.yml/badge.svg)](https://github.com/swarmic/SwarmTorch/actions/workflows/rust.yml)

**Mission-grade swarm learning for heterogeneous fleets‚ÄîRust-native, with a `no_std` core and Byzantine-resilient security posture.**

SwarmTorch is a distributed machine learning framework designed for real-world edge deployments where devices are resource-constrained, connections are unreliable, and trust cannot be assumed. Built in Rust from the ground up, it brings swarm intelligence and federated learning primitives to environments where PyTorch and TensorFlow cannot operate: embedded microcontrollers, intermittent networks, and adversarial conditions.

## Canonical Docs

- [`SWARM_TORCH_TECHNICAL_WHITE_PAPER_v0.1.md`](SWARM_TORCH_TECHNICAL_WHITE_PAPER_v0.1.md) (system model + conformance)
- [`ADRs.md`](ADRs.md) (architecture decisions)
- [`CONTEXT_SOURCES.md`](CONTEXT_SOURCES.md) (document hierarchy; drift controls)
- [`SECURITY.md`](SECURITY.md) (security policy + supply chain gates)

## Implementation Reality (2026-02-13)

- `swarm-torch` (top-level crate) is currently **std-only**.
- `swarm-torch-core` provides the portable baseline and builds under:
  - `no_std + alloc` (supported)
  - minimal `no_std` build (compiles, limited utility)
- Replay/auth enforcement is implemented (`M4-02`, `M4-02.5`).
- Concrete TCP/UDP/BLE/LoRa/WiFi transport implementations are still planned (current net path is trait + mock transport).

-----

## Why SwarmTorch?

The Rust ML ecosystem has excellent tools for specific problems:

- **[Burn](https://github.com/tracel-ai/burn)** and **[Candle](https://github.com/huggingface/candle)** for training and inference
- **[tch-rs](https://github.com/LaurentMazare/tch-rs)** for LibTorch interop
- **[tract](https://github.com/sonos/tract)** for efficient inference

**SwarmTorch adds the missing pieces:** distributed swarm learning primitives that work across heterogeneous fleets‚Äîfrom ESP32 microcontrollers to cloud servers‚Äîwith built-in Byzantine fault tolerance, supply chain integrity, useful data-wrangling automations and visualizations, and real-world network assumptions.

### What Makes SwarmTorch Different

|Feature                        |SwarmTorch               |PyTorch Distributed |TensorFlow Federated    |
|-------------------------------|-------------------------|--------------------|------------------------|
|**Embedded targets** (`no_std`)|‚ö†Ô∏è Partial (`no_std + alloc`); `embedded_min` planned|‚ùå Not supported     |‚ùå Not supported         |
|**Asynchronous participation** |‚úÖ Core design            |‚ö†Ô∏è Limited           |‚ö†Ô∏è Experimental          |
|**Byzantine robustness**       |‚úÖ Pluggable aggregators  |‚ùå No defense        |‚ö†Ô∏è Research only         |
|**Heterogeneous networks**     |‚ö†Ô∏è Trait + mock transport (concrete transports planned)|‚ùå Assumes datacenter|‚ùå Assumes reliable links|
|**Memory footprint**           |‚úÖ <256KB for participants|‚ùå GBs required      |‚ùå GBs required          |
|**Zero Python runtime**        |‚úÖ Pure Rust              |‚ùå Python required   |‚ùå Python required       |

-----

## Quick Start

### Prerequisites

```bash
# Install Rust (1.81+ for `swarm-torch`; core path supports lower MSRV)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# For embedded targets (optional)
cargo install probe-rs-tools
rustup target add thumbv7em-none-eabihf
```

### Hello Swarm (Tier 1: Desktop/Server)

Train a simple model across 3 local nodes with gossip topology:

The snippets below are **design-target pseudocode** for planned high-level APIs.
Current public APIs are more limited than these examples.

```rust,ignore
use swarm_torch::prelude::*;
use tokio;

#[tokio::main]
async fn main() -> Result<()> {
    // Define swarm cluster with 3 nodes
    let swarm = SwarmCluster::builder()
        .topology(Topology::gossip(fanout: 2))
        .consensus(RobustAggregation::trimmed_mean(trim_ratio: 0.2))
        .transport(TcpTransport::local_cluster(num_nodes: 3))
        .build()
        .await?;

    // Define model (using Burn backend)
    let model = SimpleNet::new(input_dim: 784, hidden_dim: 128, output_dim: 10);

    // Particle Swarm Optimizer for distributed training
    let optimizer = ParticleSwarm::new()
        .particles(50)
        .inertia(0.7)
        .fitness(|model, data| cross_entropy_loss(model, data));

    // Train across swarm
    let trained_model = swarm
        .train(model, optimizer, local_mnist_data())
        .max_rounds(100)
        .convergence_threshold(0.01)
        .on_round(|round, metrics| {
            println!("Round {}: loss={:.4}, accuracy={:.2}%", 
                round, metrics.loss, metrics.accuracy * 100.0);
        })
        .await?;

    Ok(())
}
```

### Hello Swarm (Tier 2: Edge Gateway)

Cross-compile for ARM Linux (e.g., Raspberry Pi):

```bash
# Add ARM target
rustup target add aarch64-unknown-linux-gnu

# Build with edge-optimized features
cargo build --release --target aarch64-unknown-linux-gnu --features edge

# Deploy to device
scp target/aarch64-unknown-linux-gnu/release/swarm-node pi@192.168.1.100:~/
```

```rust,ignore
use swarm_torch::prelude::*;

#[tokio::main]
async fn main() -> Result<()> {
    // Edge gateway coordinating IoT devices
    let swarm = SwarmCluster::builder()
        .topology(Topology::hierarchical(layers: 2))
        .transport(MultiTransport::new()
            .add_backend(WiFiTransport::new("192.168.1.0/24"))
            .add_backend(BleTransport::new(scan_window: Duration::from_secs(5))))
        .role(NodeRole::Gateway)
        .build()
        .await?;

    // Lightweight gradient aggregation for IoT nodes
    swarm.serve_aggregator().await?;
    Ok(())
}
```

### Hello Swarm (Tier 3: Embedded/`no_std`)

Minimal example for ESP32 or STM32 microcontrollers:

```rust,ignore
#![no_std]
#![no_main]

use embassy_executor::Spawner;
use swarm_torch_core::*; // no_std-compatible core

#[embassy_executor::main]
async fn main(spawner: Spawner) {
    let participant = SwarmParticipant::new()
        .role(ParticipantRole::Contributor)
        .memory_limit(64.kb())
        .transport(LoRaTransport::new(frequency: 915_000_000));

    // Compute compressed gradient sketch on device
    loop {
        let local_update = participant.compute_update(sensor_data()).await;
        participant.send_update(local_update).await;
        embassy_time::Timer::after_secs(60).await;
    }
}
```

**Note:** Full training on microcontrollers requires `alloc` feature. The `no_std` core provides gradient computation and communication primitives only.

-----

## Supported Targets & Embedded Profiles

SwarmTorch defines three explicit **embedded profiles** as portability targets (see [ADR-0002](ADRs.md#adr-0002-crate-topology-feature-flag-policy-and-embedded-profiles)). Current status: `embedded_alloc` is experimental; `embedded_min` remains planned.

|Profile                        |Feature Flags            |Allocator |Types Available                      |Examples                 |
|-------------------------------|-------------------------|----------|-------------------------------------|-------------------------|
|**`edge_std`**                 |`std`                    |Standard  |Full Rust std library                |Raspberry Pi, Jetson Nano|
|**`embedded_alloc`**           |`no_std` + `alloc`       |Required  |`Vec<u8>`, `Box<dyn Trait>`, `Arc`   |ESP32, STM32H7 (‚â•256KB)  |
|**`embedded_min`**             |`no_std`, no `alloc`     |None      |`heapless::Vec<T,N>`, fixed arrays   |Cortex-M0+, nRF52 (‚â§64KB)|

**Target compatibility matrix:**

|Target Class                   |Examples                 |Status                 |Profile           |
|-------------------------------|-------------------------|-----------------------|------------------|
|**Server/Desktop**             |`x86_64-*`, `aarch64-*`  |‚úÖ Full support         |`edge_std`        |
|**Edge Gateways**              |Raspberry Pi, Jetson Nano|‚úÖ Full support         |`edge_std`        |
|**Embedded (std)**             |OpenWRT routers          |‚úÖ Full support         |`edge_std`        |
|**Embedded (no_std + alloc)**  |ESP32, STM32H7           |‚ö†Ô∏è Experimental         |`embedded_alloc`  |
|**Embedded (no_std, no alloc)**|Cortex-M0+               |üß≠ Planned (no-alloc profile)|`embedded_min`    |

### Cross-Compilation Setup

SwarmTorch uses **[probe-rs](https://probe.rs/)** as the golden path for embedded development:

```bash
# Install probe-rs toolchain
cargo install probe-rs-tools cargo-embed

# Flash and debug embedded target
cd examples/embedded
cargo embed --chip STM32H743ZITx --release
```

See the [Supported Targets](#supported-targets--embedded-profiles) section above for detailed target-specific setup.

-----

## Feature Flags

SwarmTorch uses Cargo features to control compilation for different environments:

```toml
[dependencies]
swarm-torch = { version = "0.1", features = ["std", "tokio-runtime"] }
```

|Feature             |Description                              |Default     |
|--------------------|-----------------------------------------|------------|
|`std`               |Enable standard library support          |‚úÖ Yes       |
|`alloc`             |Enable allocator for dynamic memory      |‚úÖ (with std)|
|`tokio-runtime`     |Use Tokio for async runtime (server/edge)|‚úÖ Yes       |
|`embassy-runtime`   |Embassy runtime adapter (placeholder/experimental)|‚ùå No  |
|`burn-backend`      |Use Burn for autodiff/training           |‚úÖ Yes       |
|`tch-backend`       |Use LibTorch via tch-rs                  |‚ùå No        |
|`python-bindings`   |Build PyO3 bindings for Python           |‚ùå No        |
|`robust-aggregation`|Byzantine-resilient aggregators          |‚úÖ Yes       |
|`lora-transport`    |LoRa networking support                  |‚ùå No        |
|`ble-transport`     |Bluetooth Low Energy support             |‚ùå No        |
|`wgpu-backend`      |GPU acceleration via WGPU (portable)     |‚ùå No        |
|`cuda`              |NVIDIA CUDA acceleration (optional)      |‚ùå No        |

**Embedded profile presets:**

```toml
# For embedded_min (no_std, no alloc)
swarm-torch-core = { version = "0.1", default-features = false }

# For embedded_alloc (no_std + alloc)  
swarm-torch-core = { version = "0.1", default-features = false, features = ["alloc"] }

# For edge_std (full std)
swarm-torch = { version = "0.1", features = ["std", "tokio-runtime"] }
```

-----

## Architecture Overview

SwarmTorch is structured in layers to maximize portability and composability.  
The diagram mixes implemented components and roadmap targets.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Application Layer (Your Code)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Swarm Learning API (design target surface)            ‚îÇ
‚îÇ   ‚Ä¢ ParticleSwarm, AntColony, Firefly optimizers        ‚îÇ
‚îÇ   ‚Ä¢ Robust aggregators (Krum, Trimmed Mean, Median)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Model Abstraction Layer                               ‚îÇ
‚îÇ   ‚Ä¢ Burn wrapper (placeholder)                          ‚îÇ
‚îÇ   ‚Ä¢ tch-rs bridge (LibTorch interop)                    ‚îÇ
‚îÇ   ‚Ä¢ Custom model traits                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Identity & Coordination Layer                         ‚îÇ
‚îÇ   ‚Ä¢ Gossip-based eventual consistency (NOT consensus)   ‚îÇ
‚îÇ   ‚Ä¢ Ed25519 identity with pluggable attestation         ‚îÇ
‚îÇ   ‚Ä¢ Byzantine-robust aggregation                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Transport Abstraction (swarm-torch-net)               ‚îÇ
‚îÇ   ‚Ä¢ Trait + mock transport (implemented)                ‚îÇ
‚îÇ   ‚Ä¢ TCP/UDP/BLE/LoRa backends (planned)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Runtime Abstraction                                   ‚îÇ
‚îÇ   ‚Ä¢ Tokio (implemented)  ‚îÇ  Embassy (placeholder)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Core Design Principles

1. **Portability First:** Same API compiles to server and embedded targets
1. **Zero-Cost Abstractions:** Trait-based design with compile-time specialization
1. **Reality-First Networking:** Assumes intermittent connections, not datacenter reliability
1. **Explicit Trust Model:** Byzantine robustness is opt-in with clear guarantees
1. **Incremental Adoption:** Works alongside existing Rust ML tools (Burn, tch-rs, tract)

### GPU Acceleration Strategy

SwarmTorch uses **WGPU as the primary GPU backend** for portable acceleration across vendors, with **CUDA as an optional feature** for maximum NVIDIA performance. See [ADR-0015](ADRs.md#adr-0015-gpu-acceleration-strategy-wgpu-first-cuda-optional).

|Backend |Portability         |Performance    |Feature Flag     |
|--------|-------------------|---------------|-----------------|
|**CPU** |‚úÖ Universal        |Baseline       |Default          |
|**WGPU**|‚úÖ Vulkan/Metal/DX12|Good           |`wgpu-backend`   |
|**CUDA**|‚ùå NVIDIA only      |Highest        |`cuda`           |

```rust,ignore
// GPU backend selection
let config = SwarmConfig::builder()
    .gpu_backend(GpuBackend::Wgpu)  // Portable default
    // .gpu_backend(GpuBackend::Cuda)  // NVIDIA acceleration
    .build();
```

**CUDA policy:** CUDA is never in `swarm-torch-core` (must remain no_std compatible). CUDA acceleration is opt-in and uses dynamic loading (user provides CUDA libs).

-----

## Byzantine Robustness & Robust Aggregation

SwarmTorch treats adversarial participants as a first-class concern. When devices can fail, lie, or be compromised, classical federated averaging breaks down.

### Supported Robust Aggregators

|Aggregator                |Attack Resistance|Computational Cost|Best For                     |
|--------------------------|-----------------|------------------|-----------------------------|
|**Coordinate-wise Median**|‚úÖ High           |Low               |Low-dimensional models       |
|**Trimmed Mean**          |‚úÖ Medium-High    |Low               |Balanced performance         |
|**Krum**                  |‚úÖ High           |Medium            |Small-medium fleets          |

`Bulyan` and `Multi-Krum` are roadmap items, not implemented in the current crate surface.

**Important caveat:** Research shows that all robust aggregators can degrade under specific attack strategies, especially in high-dimensional settings. SwarmTorch provides:

- **Transparent telemetry:** See why updates are rejected
- **Attack harnesses:** Test your aggregator against known attacks
- **Topology-aware weighting:** Use network structure to improve robustness

```rust,ignore
use swarm_torch::RobustAggregation;

let strategy = RobustAggregation::TrimmedMean { trim_ratio: 0.2 };
let _ = strategy;
```

See [SECURITY.md](SECURITY.md) for detailed threat model and **[ADR-0007: Robust Aggregation Strategy](ADRs.md#adr-0007-robust-aggregation-strategy)** for design rationale.

-----

## Network Transports

SwarmTorch provides a unified `SwarmTransport` trait.  
Current implementation includes mock transport/network for integration tests; concrete TCP/UDP/BLE/LoRa/WiFi backends are roadmap work.

```rust,ignore
pub trait SwarmTransport {
    async fn send(&self, peer: PeerId, msg: &[u8]) -> Result<()>;
    async fn recv(&self) -> Result<(PeerId, Vec<u8>)>;
    async fn broadcast(&self, msg: &[u8]) -> Result<()>;
    fn reliability_class(&self) -> ReliabilityClass; // BestEffort | AtLeastOnce | Reliable
}
```

### Transport Classes (Roadmap Backends)

|Transport|Reliability|Bandwidth     |Range  |Use Case                      |
|---------|-----------|--------------|-------|------------------------------|
|**TCP**  |Reliable   |High (Gbps)   |LAN/WAN|Datacenter, edge gateways     |
|**UDP**  |Best-effort|High (Gbps)   |LAN/WAN|High-throughput, loss-tolerant|
|**WiFi** |Reliable   |Medium (Mbps) |100m   |IoT, robotics                 |
|**BLE**  |Best-effort|Low (Kbps)    |10m    |Wearables, sensors            |
|**LoRa** |Best-effort|Very low (bps)|10km+  |Remote sensors, agriculture   |

**Multi-transport design-target example:**

```rust,ignore
let transport = MultiTransport::new()
    .add(WiFiTransport::new("192.168.1.0/24"), priority: 1)
    .add(LoRaTransport::new(freq: 915_000_000), priority: 2)
    .fallback_policy(FallbackPolicy::PreferLowLatency);
```

Envelope serialization is provided by the protocol module (`postcard`).  
Concrete transport-specific framing/compression policies remain planned.  
See **[ADR-0004: Network Transport Layer](ADRs.md#adr-0004-network-transport-layer)**.

-----

## Examples & Demos

Roadmap example themes are listed below for planned scenario coverage.
Only a subset is currently checked into this repository.

### Phase 1 Themes: Robotics & ROS2 (planned)

- Multi-robot policy learning
- Topology comparison (ring vs mesh vs hierarchical)
- Byzantine robot injection scenario

### Phase 2 Themes: Edge & Embedded ML (planned)

- ESP32 contributor node
- Multi-tier quantization
- On-device continual learning

### Phase 3 Themes: Privacy & Compliance (planned)

- Audit trail demo
- Cross-org swarm training
- Differential privacy scenario

### Python Interoperability (planned)

> **‚ö†Ô∏è Model Zoo Constraint:** Python interop is limited to a **supported model zoo** with explicit portability contracts‚ÄîNOT arbitrary PyTorch model conversion. See [ADR-0009](ADRs.md#adr-0009-python-interoperability-pyo3-and-model-portability-contract).

- Jupyter notebook workflow
- Model zoo reference

Currently available examples in-repo:

```bash
cargo run -p swarm-torch --example hello_swarm
cargo run -p swarm-torch --example artifact_pipeline
```

-----

## Roadmap

### v0.1.0 - Swarm Robotics MVP (Current)

- ‚ö†Ô∏è Core algorithm configuration/types (full execution engine is in progress)
- ‚ùå TCP/UDP concrete transports (planned; trait + mock currently implemented)
- ‚úÖ Basic robust aggregators (Median, Trimmed Mean)
- ‚ùå Burn backend integration (placeholder wrapper currently)
- ‚è≥ ROS2 bridge (beta)
- ‚è≥ 5 reference robotics examples

### v0.2.0 - Edge ML Toolkit

- ‚è≥ Embassy runtime support for `no_std` targets
- ‚è≥ BLE and WiFi transport implementations
- ‚è≥ Compressed gradient protocols (TopK, randomized sparsification)
- ‚è≥ Memory-bounded participant roles (<256KB)
- ‚è≥ ESP32 and STM32 reference implementations

### v0.3.0 - Production Hardening

- ‚è≥ LoRa transport with duty-cycle management
- ‚è≥ Advanced Byzantine defense (Krum, Bulyan, Multi-Krum)
- ‚è≥ Attack simulation harness
- ‚è≥ tch-rs backend for PyTorch model compatibility (model zoo only)
- ‚è≥ ONNX export for inference interchange (import for inference, NOT training)

### v1.0.0 - Enterprise Ready

- ‚è≥ Audit logging and reproducibility tooling
- ‚è≥ Compliance mode (HIPAA, GDPR)
- ‚è≥ Python bindings (PyO3) with zero-copy data exchange
- ‚è≥ Formal verification of core protocols (TLA+)
- ‚è≥ Commercial support and SLA

See [ROADMAP.md](ROADMAP.md) for detailed milestones.

-----

## Limitations & Non-Goals (v1.0)

To maintain focus and prevent scope creep, SwarmTorch explicitly **does not aim to**:

‚ùå **Import arbitrary PyTorch models** - We support a **model zoo** with explicit portability contracts, not arbitrary conversion ([ADR-0009](ADRs.md#adr-0009-python-interoperability-pyo3-and-model-portability-contract))  
‚ùå **Use ONNX for training interchange** - ONNX is **inference-only**; training happens in SwarmTorch-native models ([ADR-0010](ADRs.md#adr-0010-model-portability-and-onnx-integration-inference-interchange))  
‚ùå **Provide Byzantine consensus** - We use gossip-based eventual consistency, NOT PBFT/Tendermint-style consensus ([ADR-0006A](ADRs.md#adr-0006a-coordination-protocol-gossip-based-eventual-consistency))  
‚ùå **Solve Sybil resistance internally** - Identity/attestation is delegated to external layer (Swarmic Network) ([ADR-0008A](ADRs.md#adr-0008a-identity-and-sybil-resistance-boundary))  
‚ùå **Support all models on all targets** - Embedded `embedded_min` profile runs participant roles only  
‚ùå **Replace existing Rust ML frameworks** - SwarmTorch complements Burn/Candle/tch-rs

### Known Limitations (v0.1)

- **Embedded support is experimental** - `no_std` core works, but tooling rough edges remain
- **No automatic hyperparameter tuning** - You must configure swarm algorithms manually
- **Limited model zoo** - Reference models only; bring your own architectures
- **Byzantine defense degrades in high dimensions** - Known issue in literature; we provide telemetry, not miracles

See [CONTRIBUTING.md](CONTRIBUTING.md) for how to help address these limitations.

-----

## Benchmarks (Pre-Release Status)

Benchmark rigor is a release gate for SwarmTorch. Cross-framework benchmark claims are temporarily withheld until a reproducible suite is published with pinned environments, scripts, and raw artifacts.

Current status:

- `cargo bench` covers internal crate-level performance checks.
- Cross-framework comparisons (for example PyTorch DDP, TensorFlow Federated) are in progress and not yet published as canonical results.
- Until the benchmark suite is published, treat performance numbers as target goals, not authoritative claims.

-----

## Contributing

SwarmTorch is open-source (MPL-2.0 license) and welcomes contributions. We especially need help with:

- ü¶Ä **Embedded drivers** - BLE, LoRa, ESP-NOW implementations
- ü§ñ **Robotics integrations** - ROS2 bridge improvements, real robot testing
- üîí **Security research** - Novel attack/defense strategies for swarm learning
- üìö **Documentation** - Tutorials, case studies, API improvements
- üß™ **Testing** - Cross-platform CI, fuzzing, property-based tests

**Before contributing:**

1. Read [CONTRIBUTING.md](CONTRIBUTING.md)
1. Check [open issues](https://github.com/swarmic/SwarmTorch/issues)
1. Review [ADRs](ADRs.md) to understand design decisions

**Code of Conduct:** We follow the [Rust Code of Conduct](https://www.rust-lang.org/policies/code-of-conduct).

-----

## Governance & Safety

### API Stability

- **v0.x:** Breaking changes allowed, but deprecated features get 2-version warning
- **v1.0+:** Semantic versioning strictly followed

### Unsafe Code Policy

- Unsafe code is permitted only in:

1. FFI boundaries (PyO3, C interop)
1. Performance-critical paths with extensive testing
1. Platform-specific optimizations (SIMD, etc.)

- All `unsafe` blocks require safety documentation and fuzzing

### Security Disclosure

Report vulnerabilities to **security@swarmtorch.dev** (see [SECURITY.md](SECURITY.md) for full disclosure policy)

**Do not** open public issues for security bugs. We follow a 90-day disclosure policy.

-----

## Citation

If you use SwarmTorch in research, please cite:

```bibtex
@software{swarmtorch2025,
  title = {SwarmTorch: Mission-Grade Swarm Learning for Heterogeneous Fleets},
  author = {SwarmTorch Contributors},
  year = {2025},
  url = {https://github.com/swarmic/SwarmTorch},
  version = {0.1.0}
}
```

**Key academic foundations:**

- Byzantine-resilient aggregation: [Blanchard et al., 2017](https://arxiv.org/abs/1703.02757)
- Gossip learning: [Heged≈±s et al., 2019](https://arxiv.org/abs/1901.08769)
- Decentralized federated learning: [Lalitha et al., 2019](https://arxiv.org/abs/1805.09063)

See [ADRs.md](ADRs.md) for full technical references.

-----

## License

SwarmTorch is licensed under the **Mozilla Public License 2.0 (MPL-2.0)**. See [LICENSE](LICENSE) for details.

**Why MPL-2.0?** File-level copyleft balances openness with protection:

- Modifications to MPL-covered files must be shared
- Can be combined with proprietary code in a "Larger Work"
- Widely adopted in Rust ecosystem (e.g., Servo, many Mozilla projects)
- Compatible with Apache-2.0 and MIT for integration

-----

## Community & Support

Public community channels are being prepared and will be linked here once live.

- üêû **Issues:** [github.com/swarmic/SwarmTorch/issues](https://github.com/swarmic/SwarmTorch/issues)
- üîê **Security Reports:** [security@swarmtorch.dev](mailto:security@swarmtorch.dev)

**Commercial Support:** Enterprise licensing, training, and consulting available via [support@swarmtorch.dev](mailto:support@swarmtorch.dev)

-----

## Acknowledgments

SwarmTorch builds on the shoulders of giants:

- **[Burn](https://github.com/tracel-ai/burn)** - Rust-native deep learning framework
- **[Embassy](https://embassy.dev/)** - Async embedded framework
- **[probe-rs](https://probe.rs/)** - Embedded debugging and flashing
- **[Tokio](https://tokio.rs/)** - Async runtime for servers/edge
- **Byzantine FL research community** - Foundational security work

Special thanks to early adopters and the SwarmTorch contributors.

-----

**Built with ü¶Ä Rust ¬∑ Designed for ü§ñ Reality ¬∑ Secured for üîí Production**
